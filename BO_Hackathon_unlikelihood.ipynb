{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "vPp-Be1BSDeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mlmS7My-R-wy"
      },
      "outputs": [],
      "source": [
        "!pip install -q pandas numpy scipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import TypeAlias, List, Tuple\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "Oog0qPAkTGZH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BO loop"
      ],
      "metadata": {
        "id": "p3RwvYNwSGcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expected_improvement(mean, std, best):\n",
        "  '''\n",
        "  EI implemented as available in this link\n",
        "  https://ekamperi.github.io/machine%20learning/2021/06/11/acquisition-functions.html#expected-improvement-ei\n",
        "  '''\n",
        "  z = (mean - best) / std\n",
        "  return (mean - best)*norm.cdf(z) + std*norm.pdf(z)\n",
        "\n",
        "def upper_confidence_bound(mean, std, best):\n",
        "  '''\n",
        "  best not used for UCB. Just keeping it here to maintain the same signature for all acquisition functions\n",
        "  '''\n",
        "  _alpha = 1.0\n",
        "  return mean + _alpha*std\n"
      ],
      "metadata": {
        "id": "xbsf_bPSSJ2W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyModel:\n",
        "  '''\n",
        "  Dummy wrapper for illustrate model training and inference\n",
        "  '''\n",
        "  def __init__(self) -> None:\n",
        "    pass\n",
        "\n",
        "  def predict(self,\n",
        "              x: List[str]\n",
        "              ) -> List[Tuple[float, float]]:\n",
        "    return [np.random.rand(2) for _ in enumerate(x)]\n",
        "\n",
        "  def train(self,\n",
        "            x: List[str],\n",
        "            y: List[int]\n",
        "            ) -> object:\n",
        "    return DummyModel()\n",
        "\n",
        "\n",
        "def create_dummy_positives():\n",
        "  '''\n",
        "  Creating a dummy pool of data with positive examples\n",
        "  '''\n",
        "  vocabulary = ['A','R','N','D','C','Q','E','G','H','I', 'L','K','M','F','P','S','T','W','Y','V']\n",
        "  sequences = [''.join(np.random.choice(vocabulary, size=50)) for _ in range(100)]\n",
        "  classes = np.random.choice([1], size=100)\n",
        "  return pd.DataFrame({'sequence': sequences, 'class': classes})\n",
        "\n",
        "\n",
        "def create_dummy_pool():\n",
        "  '''\n",
        "  Creating a dummy pool of data\n",
        "  '''\n",
        "  vocabulary = ['A','R','N','D','C','Q','E','G','H','I', 'L','K','M','F','P','S','T','W','Y','V']\n",
        "  sequences = [''.join(np.random.choice(vocabulary, size=50)) for _ in range(100)]\n",
        "  # classes = np.random.choice([0, 1], size=100) # No clases for unlabeled data\n",
        "  return pd.DataFrame(\n",
        "          {\n",
        "          'sequence': sequences,\n",
        "          # 'class': classes\n",
        "          }\n",
        "      )\n",
        "\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://github.com/ur-whitelab/peptide-dashboard/raw/master/ml/data/hemo-positive.npz\",\n",
        "    \"positive.npz\",\n",
        ")\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://github.com/ur-whitelab/peptide-dashboard/raw/master/ml/data/hemo-negative.npz\",\n",
        "    \"negative.npz\",\n",
        ")\n",
        "\n",
        "with np.load(\"positive.npz\") as r:\n",
        "    pos_data = r[list(r.keys())[0]]\n",
        "with np.load(\"negative.npz\") as r:\n",
        "    neg_data = r[list(r.keys())[0]]\n",
        "\n",
        "def build_fakes(n, data, max_length=200):\n",
        "    result = []\n",
        "    for _ in range(n):\n",
        "        # sample this many subsequences\n",
        "        avg_data_lengths = np.mean(np.count_nonzero(data, axis=1))\n",
        "        k = np.clip(np.random.poisson(avg_data_lengths), 1, len(data) - 1)\n",
        "        from scipy.stats import norm\n",
        "        k = np.clip(np.random.poisson(1), 0, len(data) - 2) + 2\n",
        "        idx = np.random.choice(range(len(data)), replace=False, size=k)\n",
        "        seq = []\n",
        "        lengths = []\n",
        "        # cut-up k into one new sequence\n",
        "        for i in range(k):\n",
        "            if np.argmin(data[idx[i]]) > 1:\n",
        "                l = np.ceil(2 * np.random.randint(1, np.argmin(data[idx[i]])) / k).astype(int)\n",
        "                lengths.append(l)\n",
        "                j = np.random.randint(0, np.argmin(data[idx[i]]) - lengths[i])\n",
        "            else:\n",
        "                lengths.append(1)\n",
        "                j = 0\n",
        "            s = data[idx[i]][j:j+lengths[i]]\n",
        "            seq.append(s)\n",
        "        seq.append([0] * (len(data[0]) - sum(lengths)))\n",
        "        # check to make sure seq length >= 1 and is valid\n",
        "        sample_seq = np.concatenate(seq)\n",
        "        if np.argmin(np.concatenate(seq)) >= 1:\n",
        "            result.append(sample_seq)\n",
        "    return np.array(result)"
      ],
      "metadata": {
        "id": "iCV17_F1Txnj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis:\n",
        "\n",
        "If the reliable negatives are well selected. The model should perform better --> Proved in the PU paper\n",
        "\n",
        "### Hypothesis 2:\n",
        "Can BO be used to robustly select good reliable negatives?"
      ],
      "metadata": {
        "id": "OILykOu5a_0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DummyModel()\n",
        "pool_u = create_dummy_pool()\n",
        "pool_p = create_dummy_positives()\n",
        "\n",
        "aqfxn = expected_improvement\n",
        "best = 0\n",
        "best_acc = 0\n",
        "n_exps = 5\n",
        "\n",
        "\n",
        "for i in range(1, n_exps+1):\n",
        "  N = i*len(pool_u)//n_exps\n",
        "  pool_u['yhat'] = model.predict(pool_u['sequence'])\n",
        "  pool_u['score'] = [aqfxn(*y, best) for y in pool_u['yhat']]\n",
        "  pool_u = pool_u.sort_values(by='score', ascending=False)\n",
        "\n",
        "  best = pool_u.iloc[0]['score']\n",
        "\n",
        "  new_dataset = pd.concat([pool_u[:N], pool_p[:N]])\n",
        "  new_dataset = new_dataset.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  model = model.train(new_dataset['sequence'], new_dataset['class'])\n",
        "\n",
        "\n",
        "model #Ideally having a good model at this point\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pvhh7d7YC19",
        "outputId": "5d42f5cb-29ec-402e-c6a8-23a09227ac53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.DummyModel at 0x7c9c538f5960>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}